{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Variational Inference algorithms for Wold processes\n",
    "\n",
    "In this notebook, we test the various variational inference (VI) algorithms implemented for the multivariate Wold process. Namely:\n",
    "\n",
    "* `WoldModelVariational`: VI with parameters $\\alpha$ and $\\beta$\n",
    "\n",
    "with the **new updates** involving solving a set of 2d^2 1-dimensional equations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load extensions for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries of interest for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set numpy print format\n",
    "np.set_printoptions(precision=2, floatmode='fixed', sign=' ')\n",
    "\n",
    "import tsvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cells width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set general experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_N_ITER = 20000\n",
    "BBVI_N_ITER = 20000\n",
    "VI_FB_N_ITER = 3000\n",
    "VI_N_ITER = 3000\n",
    "GB_N_ITER = 300\n",
    "\n",
    "VI_TOL = 1e-4\n",
    "\n",
    "PRINT_EVERY = 100\n",
    "PRINT_EVERY_VI = 10\n",
    "CALLBACK_END = '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.randint(2**32-1)\n",
    "gen_seed = None\n",
    "sim_seed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Generate toy example dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1. Small 2-dimensional toy setting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define random parameters\n",
    "dim = 2  # Dimensionality of the process\n",
    "max_jumps= 5e3 * dim  # Number of events\n",
    "\n",
    "mu = torch.tensor([0.3, 0.1])\n",
    "beta = torch.tensor([\n",
    "    [1.0, 0.2],\n",
    "    [0.5, 0.1]\n",
    "])\n",
    "# Use the same constraint as GrangerBusca to allow fair comparison\n",
    "alpha = torch.tensor([\n",
    "    [0.7, 0.3],\n",
    "    [0.0, 1.0]\n",
    "])\n",
    "\n",
    "print('Baseline:')\n",
    "print(mu.numpy().round(2))\n",
    "print('Alpha:')\n",
    "print(alpha.numpy().round(2))\n",
    "print('Beta:')\n",
    "print(beta.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2. Larger setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "[ 0.05  0.03  0.03  0.01  0.02  0.03  0.02  0.04  0.01  0.05]\n",
      "Alpha:\n",
      "[[ 0.00  0.18  0.00  0.18  0.13  0.14  0.12  0.13  0.00  0.17]\n",
      " [ 0.13  0.00  0.13  0.00  0.00  0.11  0.00  0.00  0.18  0.00]\n",
      " [ 0.11  0.00  0.00  0.00  0.11  0.19  0.19  0.16  0.00  0.11]\n",
      " [ 0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.16  0.00  0.12]\n",
      " [ 0.00  0.00  0.11  0.00  0.14  0.00  0.17  0.19  0.00  0.12]\n",
      " [ 0.00  0.17  0.00  0.00  0.17  0.00  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.12  0.00  0.00  0.13  0.00  0.12  0.00  0.00  0.16]\n",
      " [ 0.14  0.00  0.00  0.12  0.00  0.18  0.00  0.12  0.00  0.00]\n",
      " [ 0.00  0.00  0.20  0.00  0.00  0.12  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.19  0.16  0.19  0.18  0.00  0.00  0.17  0.20  0.00]]\n",
      "Beta:\n",
      "[[ 0.05  0.80  0.11  0.39  0.18  0.94  0.81  0.42  0.88  0.24]\n",
      " [ 0.50  0.64  0.26  0.14  0.68  0.69  0.76  0.16  0.15  0.49]\n",
      " [ 0.31  0.51  0.40  0.89  0.35  0.95  0.47  0.19  0.36  0.64]\n",
      " [ 0.32  0.21  0.14  0.16  0.95  0.68  0.72  0.95  0.74  0.21]\n",
      " [ 0.56  0.66  0.21  0.20  0.40  0.04  0.88  0.68  0.10  0.53]\n",
      " [ 0.88  0.99  0.37  0.47  0.97  0.99  0.42  0.72  0.44  0.22]\n",
      " [ 0.08  0.96  0.17  0.09  0.56  0.78  0.93  0.54  0.29  0.07]\n",
      " [ 0.21  0.42  0.28  0.13  0.36  0.39  0.70  0.66  0.71  0.77]\n",
      " [ 0.71  0.46  0.00  0.79  0.29  0.54  0.42  0.07  0.18  0.72]\n",
      " [ 0.85  0.65  0.17  0.92  0.05  0.96  0.59  0.21  0.73  0.85]]\n"
     ]
    }
   ],
   "source": [
    "from experiments_utils import generate_parameters\n",
    "\n",
    "dim = 10\n",
    "max_jumps=7e3 * dim\n",
    "\n",
    "param_dict = generate_parameters(dim=dim, seed=gen_seed)\n",
    "\n",
    "mu = torch.tensor(param_dict['baseline'])\n",
    "beta = torch.tensor(param_dict['beta'])\n",
    "alpha = torch.tensor(param_dict['adjacency'])\n",
    "\n",
    "print('Baseline:')\n",
    "print(mu.numpy().round(2))\n",
    "print('Alpha:')\n",
    "print(alpha.numpy().round(2))\n",
    "print('Beta:')\n",
    "print(beta.numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Simulate lots of data...\n",
      "    - Simulated 70,000 events with end time: 45202.078125\n",
      "    - Events:\n",
      "      - dim  0 ( 5857 events): [    66.31     71.67     80.19 ...  45169.36  45174.48  45190.67]\n",
      "      - dim  1 ( 6870 events): [    50.76     56.15    111.64 ...  45167.43  45181.51  45182.12]\n",
      "      - dim  2 ( 6915 events): [   115.64    121.77    125.21 ...  45192.52  45194.21  45200.74]\n",
      "      - dim  3 ( 4730 events): [     7.91     98.04    120.52 ...  45167.24  45194.52  45196.43]\n",
      "      - dim  4 ( 8959 events): [    54.71    131.31    131.84 ...  45187.09  45189.07  45193.11]\n",
      "      - dim  5 ( 8279 events): [    17.33     17.98     71.09 ...  45190.25  45192.10  45202.08]\n",
      "      - dim  6 ( 5662 events): [   135.03    135.93    143.69 ...  45177.10  45194.96  45200.58]\n",
      "      - dim  7 (10705 events): [     5.61      9.96     13.69 ...  45184.52  45186.82  45198.65]\n",
      "      - dim  8 ( 3617 events): [   175.37    176.77    179.58 ...  45155.62  45169.08  45172.67]\n",
      "      - dim  9 ( 8406 events): [    81.20     84.31     90.60 ...  45194.33  45195.91  45199.15]\n"
     ]
    }
   ],
   "source": [
    "coeffs_true_dict = {\n",
    "    'baseline': mu.numpy(),\n",
    "    'adjacency': alpha.numpy(),\n",
    "    'beta': beta.numpy()\n",
    "}\n",
    "\n",
    "coeffs_true = torch.cat((mu, beta.flatten(), alpha.flatten())).numpy()\n",
    "print('  - Simulate lots of data...')\n",
    "# Simulate lots of data\n",
    "wold_sim = tsvar.simulate.MultivariateWoldSimulator(\n",
    "    mu_a=mu, alpha_ba=alpha, beta_ba=beta)\n",
    "events = wold_sim.simulate(max_jumps=max_jumps, seed=sim_seed)\n",
    "events = [torch.tensor(ev, dtype=torch.float) for ev in events]\n",
    "end_time = max(map(max, events))\n",
    "print((f\"    - Simulated {sum(map(len, events)):,d} events \"\n",
    "       f\"with end time: {end_time}\"))\n",
    "print(\"    - Events:\")\n",
    "for i, events_i in enumerate(events):\n",
    "    print(f\"      - dim {i:>2d} ({len(events_i):>5d} events):\", events_i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:   100 | dx: +4.9863e-02 | loss: 3.1766e+00 | dloss: -9.12e-05 | f1-score: 0.34 | relerr: 5.36e-01 | time/it: 3.30e-02     \n",
      "iter:   200 | dx: +3.4440e-02 | loss: 3.1700e+00 | dloss: -4.57e-05 | f1-score: 0.34 | relerr: 5.27e-01 | time/it: 2.80e-02     \n",
      "iter:   300 | dx: +2.8639e-02 | loss: 3.1663e+00 | dloss: -3.21e-05 | f1-score: 0.34 | relerr: 5.19e-01 | time/it: 2.71e-02     \n",
      "iter:   400 | dx: +2.5257e-02 | loss: 3.1634e+00 | dloss: -2.61e-05 | f1-score: 0.34 | relerr: 5.17e-01 | time/it: 2.74e-02     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-65e856ab2f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                  \u001b[0mlr_sched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMLE_N_ITER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                  \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianPrior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                  seed=None, callback=callback)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mcoeffs_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_wold.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmle_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/fitter.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, objective_func, x0, optimizer, lr, lr_sched, tol, max_iter, penalty, C, seed, callback)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_iter_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_gradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;31m# Check that the optimization did not fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/fitter.py\u001b[0m in \u001b[0;36m_take_gradient_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_wold.py\u001b[0m in \u001b[0;36mmle_objective\u001b[0;34m(self, coeffs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmle_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m\"\"\"Objectvie function for MLE: Averaged negative log-likelihood\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jumps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_wold.py\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(self, coeffs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             lam_ik_arr = mu[i] + torch.sum(\n\u001b[1;32m    242\u001b[0m                 self.valid_mask_ikj[i] * alpha[:, i] / (\n\u001b[0;32m--> 243\u001b[0;31m                     beta[:, i] + 1 + self.delta_ikj[i]), axis=1)\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Add the log-intensity term (ignore the last 'fake' event)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mlog_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlam_ik_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim = len(events)\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set initial guess\n",
    "coeffs_start = torch.tensor(np.hstack((\n",
    "    np.random.uniform(0.0, 1.0, size=dim),     # baseline\n",
    "    np.random.uniform(0.0, 1.0, size=dim**2),  # beta\n",
    "    np.random.uniform(0.0, 1.0, size=dim**2)   # adjacency\n",
    ")))\n",
    "\n",
    "# Extract ground truth\n",
    "coeffs_true = np.hstack((coeffs_true_dict['baseline'],\n",
    "                         np.array(coeffs_true_dict['beta']).flatten(),\n",
    "                         np.array(coeffs_true_dict['adjacency']).flatten()))\n",
    "\n",
    "# Define model\n",
    "model = tsvar.models.WoldModelMLE(verbose=True)\n",
    "model.observe(events, end_time)\n",
    "\n",
    "# Set callback\n",
    "callback = tsvar.utils.callbacks.LearnerCallbackMLE(\n",
    "    coeffs_start, print_every=PRINT_EVERY, coeffs_true=coeffs_true,\n",
    "    acc_thresh=0.05, dim=dim, default_end=CALLBACK_END)\n",
    "\n",
    "# Fit model\n",
    "conv = model.fit(x0=coeffs_start, optimizer=torch.optim.Adam, lr=0.1,\n",
    "                 lr_sched=0.9999, tol=1e-4, max_iter=MLE_N_ITER,\n",
    "                 penalty=tsvar.priors.GaussianPrior, C=1e10,\n",
    "                 seed=None, callback=callback)\n",
    "coeffs_hat = model.coeffs.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test BBVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:   100 | dx: +3.9746e-02 | loss: 8.4872e+02 | dloss: +3.59e+01 | f1-score: 0.6014 | relerr: 2.70e+00 | time/it: 3.56e-02     \n",
      "iter:   200 | dx: +4.8166e-02 | loss: 6.4354e+02 | dloss: -1.39e+02 | f1-score: 0.6014 | relerr: 2.36e+00 | time/it: 4.01e-02     \n",
      "iter:   300 | dx: +4.0195e-02 | loss: 6.1814e+02 | dloss: -2.57e+01 | f1-score: 0.6014 | relerr: 2.21e+00 | time/it: 3.33e-02     \n",
      "iter:   400 | dx: +5.1695e-02 | loss: 5.6276e+02 | dloss: -3.10e+01 | f1-score: 0.6014 | relerr: 2.15e+00 | time/it: 3.48e-02     \n",
      "iter:   500 | dx: +4.7395e-02 | loss: 5.1590e+02 | dloss: +2.60e+01 | f1-score: 0.6014 | relerr: 2.12e+00 | time/it: 3.62e-02     \n",
      "iter:   600 | dx: +8.6757e-02 | loss: 6.2736e+02 | dloss: +1.04e+02 | f1-score: 0.6014 | relerr: 2.24e+00 | time/it: 3.40e-02     \n",
      "iter:   700 | dx: +8.2242e-02 | loss: 5.5800e+02 | dloss: -8.01e+02 | f1-score: 0.6014 | relerr: 2.31e+00 | time/it: 3.38e-02     \n",
      "iter:   800 | dx: +4.1890e-02 | loss: 5.7970e+02 | dloss: +4.63e+01 | f1-score: 0.6014 | relerr: 2.42e+00 | time/it: 3.55e-02     \n",
      "iter:   900 | dx: +4.0843e-02 | loss: 5.2362e+02 | dloss: -8.96e+00 | f1-score: 0.6014 | relerr: 2.61e+00 | time/it: 3.55e-02     \n",
      "iter:  1000 | dx: +2.6730e-02 | loss: 5.5737e+02 | dloss: +3.81e+01 | f1-score: 0.6014 | relerr: 2.84e+00 | time/it: 3.34e-02     \n",
      "iter:  1100 | dx: +5.6784e-02 | loss: 5.9277e+02 | dloss: +6.91e+01 | f1-score: 0.6014 | relerr: 3.02e+00 | time/it: 3.36e-02     \n",
      "iter:  1200 | dx: +4.2044e-02 | loss: 5.0768e+02 | dloss: +2.88e+01 | f1-score: 0.6014 | relerr: 3.15e+00 | time/it: 3.50e-02     \n",
      "iter:  1300 | dx: +4.6174e-02 | loss: 5.4099e+02 | dloss: -8.40e+01 | f1-score: 0.6014 | relerr: 3.25e+00 | time/it: 3.46e-02     \n",
      "iter:  1400 | dx: +7.3442e-02 | loss: 5.7039e+02 | dloss: -4.65e+01 | f1-score: 0.6014 | relerr: 3.24e+00 | time/it: 3.38e-02     \n",
      "iter:  1500 | dx: +6.8777e-02 | loss: 5.6866e+02 | dloss: -7.01e+00 | f1-score: 0.6014 | relerr: 3.22e+00 | time/it: 3.38e-02     \n",
      "iter:  1600 | dx: +8.6514e-02 | loss: 5.8084e+02 | dloss: -8.11e+01 | f1-score: 0.6014 | relerr: 3.15e+00 | time/it: 3.43e-02     \n",
      "iter:  1700 | dx: +4.3670e-02 | loss: 5.1378e+02 | dloss: -6.71e+00 | f1-score: 0.6014 | relerr: 2.91e+00 | time/it: 3.40e-02     \n",
      "iter:  1800 | dx: +3.2920e-02 | loss: 5.5016e+02 | dloss: +1.90e+01 | f1-score: 0.6014 | relerr: 2.69e+00 | time/it: 3.41e-02     \n",
      "iter:  1900 | dx: +3.2656e-02 | loss: 5.8096e+02 | dloss: +1.56e+01 | f1-score: 0.6014 | relerr: 2.34e+00 | time/it: 3.86e-02     \n",
      "iter:  2000 | dx: +4.6619e-02 | loss: 5.8293e+02 | dloss: -2.26e+01 | f1-score: 0.6014 | relerr: 2.08e+00 | time/it: 3.30e-02     \n",
      "iter:  2100 | dx: +4.3571e-02 | loss: 7.1827e+02 | dloss: +1.21e+02 | f1-score: 0.6014 | relerr: 1.83e+00 | time/it: 3.47e-02     \n",
      "iter:  2200 | dx: +2.9343e-02 | loss: 5.4037e+02 | dloss: -5.06e+00 | f1-score: 0.6014 | relerr: 1.63e+00 | time/it: 3.35e-02     \n",
      "iter:  2300 | dx: +4.1504e-02 | loss: 5.9731e+02 | dloss: +2.20e+01 | f1-score: 0.6056 | relerr: 1.47e+00 | time/it: 3.32e-02     \n",
      "iter:  2400 | dx: +3.3721e-02 | loss: 5.9215e+02 | dloss: +4.17e+01 | f1-score: 0.6056 | relerr: 1.35e+00 | time/it: 3.34e-02     \n",
      "iter:  2500 | dx: +3.5934e-02 | loss: 7.0715e+02 | dloss: +5.62e+01 | f1-score: 0.6187 | relerr: 1.26e+00 | time/it: 3.38e-02     \n",
      "iter:  2600 | dx: +2.9770e-02 | loss: 5.9918e+02 | dloss: -1.05e+01 | f1-score: 0.6222 | relerr: 1.15e+00 | time/it: 4.18e-02     \n",
      "iter:  2700 | dx: +3.3625e-02 | loss: 5.9929e+02 | dloss: +1.62e+01 | f1-score: 0.6277 | relerr: 1.03e+00 | time/it: 3.47e-02     \n",
      "iter:  2800 | dx: +4.2745e-02 | loss: 6.2338e+02 | dloss: +2.57e+01 | f1-score: 0.6466 | relerr: 9.86e-01 | time/it: 3.42e-02     \n",
      "iter:  2900 | dx: +3.2781e-02 | loss: 6.8043e+02 | dloss: +5.48e+01 | f1-score: 0.6466 | relerr: 9.40e-01 | time/it: 3.37e-02     \n",
      "iter:  3000 | dx: +2.5099e-02 | loss: 6.4036e+02 | dloss: +1.51e+01 | f1-score: 0.6316 | relerr: 9.17e-01 | time/it: 3.45e-02     \n",
      "iter:  3100 | dx: +5.5377e-02 | loss: 6.3043e+02 | dloss: -9.01e+01 | f1-score: 0.6613 | relerr: 8.57e-01 | time/it: 3.37e-02     \n",
      "iter:  3200 | dx: +3.1220e-02 | loss: 6.5800e+02 | dloss: -8.06e+01 | f1-score: 0.6721 | relerr: 7.99e-01 | time/it: 3.38e-02     \n",
      "iter:  3300 | dx: +1.8070e-02 | loss: 7.1720e+02 | dloss: +8.18e+01 | f1-score: 0.7059 | relerr: 7.77e-01 | time/it: 3.72e-02     \n",
      "iter:  3400 | dx: +3.5766e-02 | loss: 7.2341e+02 | dloss: +5.42e+00 | f1-score: 0.6838 | relerr: 7.26e-01 | time/it: 3.45e-02     \n",
      "iter:  3500 | dx: +3.2787e-02 | loss: 7.5379e+02 | dloss: +1.33e+01 | f1-score: 0.6833 | relerr: 6.93e-01 | time/it: 3.36e-02     \n",
      "iter:  3600 | dx: +5.6876e-02 | loss: 6.8653e+02 | dloss: -5.67e+01 | f1-score: 0.7241 | relerr: 6.37e-01 | time/it: 3.39e-02     \n",
      "iter:  3700 | dx: +1.9081e-02 | loss: 7.3132e+02 | dloss: +3.35e+01 | f1-score: 0.7321 | relerr: 5.93e-01 | time/it: 5.17e-02     \n",
      "iter:  3800 | dx: +5.8965e-02 | loss: 7.4847e+02 | dloss: +1.50e+01 | f1-score: 0.7273 | relerr: 5.57e-01 | time/it: 4.39e-02     \n",
      "iter:  3900 | dx: +3.3011e-02 | loss: 7.5402e+02 | dloss: +8.87e+00 | f1-score: 0.7921 | relerr: 5.21e-01 | time/it: 3.21e-02     \n",
      "iter:  4000 | dx: +2.7683e-02 | loss: 7.9397e+02 | dloss: +2.90e+01 | f1-score: 0.8247 | relerr: 4.92e-01 | time/it: 3.43e-02     \n",
      "iter:  4100 | dx: +1.4985e-02 | loss: 7.5929e+02 | dloss: -6.10e+01 | f1-score: 0.8298 | relerr: 4.67e-01 | time/it: 4.19e-02     \n",
      "iter:  4200 | dx: +5.4025e-02 | loss: 7.9737e+02 | dloss: +8.17e+00 | f1-score: 0.8125 | relerr: 4.55e-01 | time/it: 4.06e-02     \n",
      "iter:  4300 | dx: +4.6987e-02 | loss: 8.1404e+02 | dloss: -1.36e+01 | f1-score: 0.8387 | relerr: 4.36e-01 | time/it: 3.68e-02     \n",
      "iter:  4400 | dx: +4.1844e-02 | loss: 8.0479e+02 | dloss: -3.71e+01 | f1-score: 0.8261 | relerr: 4.34e-01 | time/it: 4.27e-02     \n",
      "iter:  4500 | dx: +3.9303e-02 | loss: 8.5486e+02 | dloss: +3.20e+01 | f1-score: 0.7912 | relerr: 4.31e-01 | time/it: 3.49e-02     \n",
      "iter:  4600 | dx: +2.5141e-02 | loss: 8.3543e+02 | dloss: +1.72e+01 | f1-score: 0.8000 | relerr: 4.16e-01 | time/it: 3.91e-02     \n",
      "iter:  4700 | dx: +2.2549e-02 | loss: 8.4386e+02 | dloss: -2.51e+01 | f1-score: 0.8000 | relerr: 4.01e-01 | time/it: 5.05e-02     \n",
      "iter:  4800 | dx: +3.0596e-02 | loss: 8.5921e+02 | dloss: -2.15e+01 | f1-score: 0.8409 | relerr: 3.89e-01 | time/it: 3.44e-02     \n",
      "iter:  4900 | dx: +5.2695e-02 | loss: 9.0528e+02 | dloss: +3.68e+01 | f1-score: 0.8506 | relerr: 3.77e-01 | time/it: 3.22e-02     \n",
      "iter:  5000 | dx: +3.7534e-02 | loss: 9.2052e+02 | dloss: -2.04e+01 | f1-score: 0.8235 | relerr: 3.72e-01 | time/it: 3.26e-02     \n",
      "iter:  5100 | dx: +2.5026e-02 | loss: 8.8456e+02 | dloss: -4.23e+01 | f1-score: 0.8571 | relerr: 3.68e-01 | time/it: 3.26e-02     \n",
      "iter:  5200 | dx: +4.4506e-02 | loss: 9.1271e+02 | dloss: -6.20e+01 | f1-score: 0.8471 | relerr: 3.69e-01 | time/it: 3.31e-02     \n",
      "iter:  5300 | dx: +3.2704e-02 | loss: 9.2880e+02 | dloss: -1.00e+01 | f1-score: 0.8333 | relerr: 3.67e-01 | time/it: 3.53e-02     \n",
      "iter:  5400 | dx: +1.3894e-02 | loss: 9.4222e+02 | dloss: -1.13e+00 | f1-score: 0.8571 | relerr: 3.73e-01 | time/it: 4.14e-02     \n",
      "iter:  5500 | dx: +2.0980e-02 | loss: 9.3168e+02 | dloss: -1.21e+01 | f1-score: 0.8571 | relerr: 3.63e-01 | time/it: 3.44e-02     \n",
      "iter:  5600 | dx: +2.5182e-02 | loss: 9.9904e+02 | dloss: +4.21e+00 | f1-score: 0.8471 | relerr: 3.59e-01 | time/it: 3.49e-02     \n",
      "iter:  5700 | dx: +5.0022e-02 | loss: 1.0047e+03 | dloss: -9.43e+00 | f1-score: 0.8471 | relerr: 3.57e-01 | time/it: 3.56e-02     \n",
      "iter:  5800 | dx: +1.9816e-02 | loss: 9.9353e+02 | dloss: -3.89e+01 | f1-score: 0.8471 | relerr: 3.57e-01 | time/it: 3.77e-02     \n",
      "iter:  5900 | dx: +3.1401e-02 | loss: 9.9466e+02 | dloss: -1.76e+01 | f1-score: 0.8471 | relerr: 3.52e-01 | time/it: 3.60e-02     \n",
      "iter:  6000 | dx: +2.7592e-02 | loss: 1.0623e+03 | dloss: +2.58e+01 | f1-score: 0.8471 | relerr: 3.45e-01 | time/it: 3.85e-02     \n",
      "iter:  6100 | dx: +3.0449e-02 | loss: 1.1191e+03 | dloss: +6.94e+01 | f1-score: 0.8571 | relerr: 3.40e-01 | time/it: 3.41e-02     \n",
      "iter:  6200 | dx: +2.5218e-02 | loss: 1.0319e+03 | dloss: -7.20e+00 | f1-score: 0.8571 | relerr: 3.39e-01 | time/it: 3.37e-02     \n",
      "iter:  6300 | dx: +3.5488e-02 | loss: 1.0740e+03 | dloss: +3.06e+01 | f1-score: 0.8571 | relerr: 3.35e-01 | time/it: 5.30e-02     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  6400 | dx: +4.9466e-02 | loss: 1.1026e+03 | dloss: +2.77e+01 | f1-score: 0.8675 | relerr: 3.35e-01 | time/it: 4.49e-02     \n",
      "iter:  6500 | dx: +1.8695e-02 | loss: 1.1026e+03 | dloss: -2.82e+00 | f1-score: 0.8675 | relerr: 3.21e-01 | time/it: 5.50e-02     \n",
      "iter:  6600 | dx: +1.7117e-02 | loss: 1.1209e+03 | dloss: +1.04e+00 | f1-score: 0.8675 | relerr: 3.13e-01 | time/it: 4.97e-02     \n",
      "iter:  6700 | dx: +3.1088e-02 | loss: 1.1119e+03 | dloss: -2.07e+01 | f1-score: 0.8675 | relerr: 3.09e-01 | time/it: 3.86e-02     \n",
      "iter:  6800 | dx: +3.2569e-02 | loss: 1.1557e+03 | dloss: -1.29e+01 | f1-score: 0.8675 | relerr: 2.99e-01 | time/it: 3.94e-02     \n",
      "iter:  6900 | dx: +2.5613e-02 | loss: 1.1797e+03 | dloss: +1.27e+01 | f1-score: 0.8675 | relerr: 3.02e-01 | time/it: 5.52e-02     \n",
      "iter:  7000 | dx: +1.5984e-02 | loss: 1.1761e+03 | dloss: +1.29e+01 | f1-score: 0.8675 | relerr: 3.01e-01 | time/it: 3.94e-02     \n",
      "iter:  7100 | dx: +4.0324e-02 | loss: 1.1809e+03 | dloss: -4.04e+01 | f1-score: 0.8675 | relerr: 2.95e-01 | time/it: 4.22e-02     \n",
      "iter:  7200 | dx: +5.0526e-02 | loss: 1.1829e+03 | dloss: -3.07e+01 | f1-score: 0.8675 | relerr: 2.94e-01 | time/it: 3.87e-02     \n",
      "iter:  7300 | dx: +1.9038e-02 | loss: 1.2259e+03 | dloss: -6.42e+00 | f1-score: 0.8675 | relerr: 2.97e-01 | time/it: 3.30e-02     \n",
      "iter:  7400 | dx: +1.6548e-02 | loss: 1.2149e+03 | dloss: -5.21e+01 | f1-score: 0.8675 | relerr: 2.98e-01 | time/it: 4.09e-02     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a91c8b0d46f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                  \u001b[0mlr_sched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBBVI_N_ITER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                  \u001b[0mmstep_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmstep_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmstep_momentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                  seed=None, callback=callback)\n\u001b[0m",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_wold_bbvi.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbvi_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/fitter.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, objective_func, x0, optimizer, lr, lr_sched, tol, max_iter, mstep_interval, mstep_offset, mstep_momentum, seed, callback)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_iter_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;31m# E step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0;31m# M step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/fitter.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_models.py\u001b[0m in \u001b[0;36mbbvi_objective\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Compute a Monte Carlo estimate of the expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m  \u001b[0;31m# Negative ELBO (we want to minimize it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_models.py\u001b[0m in \u001b[0;36m_objective_l\u001b[0;34m(self, eps_l, alpha, beta)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Compute the importance weights (and their gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mlog_w_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_importance_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Temper the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mlog_w_arr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_models.py\u001b[0m in \u001b[0;36m_log_importance_weight\u001b[0;34m(self, eps, alpha, beta)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Evaluate the log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Compute the log-prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mlogprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             raise Exception(('Model is not properly set. '\n",
      "\u001b[0;32m~/Workspace/EPFL/research/ongoing/var-wold/tsvar/models/_wold.py\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(self, coeffs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             lam_ik_arr = mu[i] + torch.sum(\n\u001b[1;32m    242\u001b[0m                 self.valid_mask_ikj[i] * alpha[:, i] / (\n\u001b[0;32m--> 243\u001b[0;31m                     beta[:, i] + 1 + self.delta_ikj[i]), axis=1)\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Add the log-intensity term (ignore the last 'fake' event)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mlog_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlam_ik_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim = len(events)\n",
    "n_params = dim + dim**2 + dim**2\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# Set initial guess\n",
    "coeffs_start = torch.tensor(np.hstack((\n",
    "    # loc\n",
    "    -2.0 * torch.ones(dim, dtype=torch.float),                  # baseline\n",
    "    0.0 * torch.ones((dim, dim), dtype=torch.float).flatten(),  # beta\n",
    "    0.0 * torch.ones((dim, dim), dtype=torch.float).flatten(),  # adjacency\n",
    "    # scale\n",
    "    torch.log(0.2 * torch.ones(dim, dtype=torch.float)),\n",
    "    torch.log(0.2 * torch.ones((dim, dim), dtype=torch.float).flatten()),\n",
    "    torch.log(0.2 * torch.ones((dim, dim), dtype=torch.float).flatten()),\n",
    ")))\n",
    "# Extract ground truth\n",
    "coeffs_true = np.hstack((coeffs_true_dict['baseline'],\n",
    "                         np.array(coeffs_true_dict['beta']).flatten(),\n",
    "                         np.array(coeffs_true_dict['adjacency']).flatten()))\n",
    "\n",
    "# Define priors/posteriors\n",
    "posterior = tsvar.posteriors.LogNormalPosterior\n",
    "prior = tsvar.priors.GaussianLaplacianPrior\n",
    "mask_gaus = torch.zeros(n_params, dtype=torch.bool)\n",
    "mask_gaus[:dim + dim**2] = 1  # Gaussian prior for baseline and beta\n",
    "C = 1e3\n",
    "\n",
    "# Init the model object\n",
    "model = tsvar.models.WoldModelBBVI(posterior=posterior, prior=prior, C=C,\n",
    "                                   prior_kwargs={'mask_gaus': mask_gaus},\n",
    "                                   n_samples=1, n_weights=1, weight_temp=1,\n",
    "                                   verbose=False, device='cpu')\n",
    "model.observe(events, end_time)\n",
    "\n",
    "# Set link function for callback (vi coeffs -> posterior mode)\n",
    "def link_func(coeffs):\n",
    "    \"\"\"variationa coeffs -> posterior mode of adjacency\"\"\"\n",
    "    # Numpy to torch\n",
    "    coeffs = torch.tensor(coeffs) if isinstance(coeffs, np.ndarray) else coeffs\n",
    "    return model.posterior.mode(\n",
    "        coeffs[:model.n_params], coeffs[model.n_params:]\n",
    "    ).detach().numpy()[dim+dim**2:]\n",
    "\n",
    "# Set the callback (callback parameters are posterior mode)\n",
    "callback = tsvar.utils.callbacks.LearnerCallbackMLE(\n",
    "    x0=posterior().mode(\n",
    "        coeffs_start[:dim+2*dim**2], coeffs_start[dim+2*dim**2:]\n",
    "    )[dim+dim**2:],\n",
    "    print_every=PRINT_EVERY,\n",
    "    coeffs_true=coeffs_true,\n",
    "    acc_thresh=0.05,\n",
    "    dim=dim,\n",
    "    link_func=link_func,\n",
    "    default_end=CALLBACK_END)\n",
    "\n",
    "# Fit the model\n",
    "conv = model.fit(x0=coeffs_start, optimizer=torch.optim.Adam, lr=1e-1,\n",
    "                 lr_sched=0.9999, tol=1e-6, max_iter=BBVI_N_ITER,\n",
    "                 mstep_interval=100, mstep_offset=500, mstep_momentum=0.5,\n",
    "                 seed=None, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test VI-fb\n",
    "\n",
    "Algorithm: Mean-Field VI with fixed $\\{\\beta\\}$s using `WoldModelVariationalFixedBeta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:    10 | dx: +7.5197e-03 | f1-score: 0.83 | relerr: 3.44e-01 | time/it: 5.17e-01     \n",
      "iter:    20 | dx: +3.6251e-03 | f1-score: 0.92 | relerr: 2.28e-01 | time/it: 2.87e-02     \n",
      "iter:    30 | dx: +1.8342e-03 | f1-score: 0.97 | relerr: 1.66e-01 | time/it: 3.25e-02     \n",
      "iter:    40 | dx: +9.8419e-04 | f1-score: 0.99 | relerr: 1.33e-01 | time/it: 2.52e-02     \n",
      "iter:    50 | dx: +6.1220e-04 | f1-score: 0.99 | relerr: 1.15e-01 | time/it: 2.33e-02     \n",
      "iter:    60 | dx: +4.5139e-04 | f1-score: 1.00 | relerr: 1.04e-01 | time/it: 2.38e-02     \n",
      "iter:    70 | dx: +3.1352e-04 | f1-score: 1.00 | relerr: 9.66e-02 | time/it: 2.83e-02     \n",
      "iter:    80 | dx: +2.5958e-04 | f1-score: 1.00 | relerr: 9.04e-02 | time/it: 2.44e-02     \n",
      "iter:    90 | dx: +2.0126e-04 | f1-score: 1.00 | relerr: 8.54e-02 | time/it: 2.20e-02     \n",
      "iter:   100 | dx: +1.7109e-04 | f1-score: 1.00 | relerr: 8.14e-02 | time/it: 2.16e-02     \n",
      "iter:   110 | dx: +1.5961e-04 | f1-score: 1.00 | relerr: 7.80e-02 | time/it: 2.19e-02     \n",
      "iter:   120 | dx: +1.4274e-04 | f1-score: 1.00 | relerr: 7.49e-02 | time/it: 2.16e-02     \n",
      "iter:   130 | dx: +1.3198e-04 | f1-score: 1.00 | relerr: 7.21e-02 | time/it: 2.16e-02     \n",
      "iter:   140 | dx: +1.2427e-04 | f1-score: 1.00 | relerr: 6.96e-02 | time/it: 2.17e-02     \n",
      "iter:   150 | dx: +1.1074e-04 | f1-score: 1.00 | relerr: 6.73e-02 | time/it: 2.17e-02     \n",
      "iter:   160 | dx: +1.0576e-04 | f1-score: 1.00 | relerr: 6.53e-02 | time/it: 2.26e-02     \n",
      "iter:   170 | dx: +9.4934e-05 | f1-score: 1.00 | relerr: 6.35e-02 | time/it: 2.22e-02     \n",
      "iter:   180 | dx: +8.5554e-05 | f1-score: 1.00 | relerr: 6.18e-02 | time/it: 2.31e-02     \n",
      "iter:   190 | dx: +8.1450e-05 | f1-score: 1.00 | relerr: 6.03e-02 | time/it: 2.16e-02     \n",
      "iter:   200 | dx: +7.2187e-05 | f1-score: 1.00 | relerr: 5.89e-02 | time/it: 2.14e-02     \n",
      "iter:   210 | dx: +7.0441e-05 | f1-score: 1.00 | relerr: 5.77e-02 | time/it: 2.13e-02     \n",
      "iter:   220 | dx: +6.1109e-05 | f1-score: 1.00 | relerr: 5.65e-02 | time/it: 2.25e-02     \n",
      "iter:   230 | dx: +5.8821e-05 | f1-score: 1.00 | relerr: 5.55e-02 | time/it: 2.14e-02     \n",
      "iter:   240 | dx: +5.5989e-05 | f1-score: 1.00 | relerr: 5.45e-02 | time/it: 2.24e-02     \n",
      "iter:   250 | dx: +5.3310e-05 | f1-score: 1.00 | relerr: 5.38e-02 | time/it: 2.19e-02     \n",
      "iter:   260 | dx: +5.3594e-05 | f1-score: 1.00 | relerr: 5.31e-02 | time/it: 2.67e-02     \n",
      "iter:   270 | dx: +4.9560e-05 | f1-score: 1.00 | relerr: 5.25e-02 | time/it: 2.19e-02     \n",
      "iter:   280 | dx: +4.7421e-05 | f1-score: 1.00 | relerr: 5.20e-02 | time/it: 2.13e-02     \n",
      "iter:   290 | dx: +4.5418e-05 | f1-score: 1.00 | relerr: 5.15e-02 | time/it: 2.17e-02     \n",
      "iter:   300 | dx: +4.5333e-05 | f1-score: 1.00 | relerr: 5.11e-02 | time/it: 2.44e-02     \n",
      "iter:   310 | dx: +4.6864e-05 | f1-score: 1.00 | relerr: 5.06e-02 | time/it: 2.16e-02     \n",
      "iter:   320 | dx: +4.8486e-05 | f1-score: 1.00 | relerr: 5.02e-02 | time/it: 2.17e-02     \n",
      "iter:   330 | dx: +5.0390e-05 | f1-score: 1.00 | relerr: 4.97e-02 | time/it: 2.15e-02     \n",
      "iter:   340 | dx: +5.2618e-05 | f1-score: 1.00 | relerr: 4.93e-02 | time/it: 2.15e-02     \n",
      "iter:   350 | dx: +5.4681e-05 | f1-score: 1.00 | relerr: 4.89e-02 | time/it: 2.21e-02     \n",
      "iter:   360 | dx: +4.8554e-05 | f1-score: 1.00 | relerr: 4.85e-02 | time/it: 2.19e-02     \n",
      "iter:   370 | dx: +3.7270e-05 | f1-score: 1.00 | relerr: 4.81e-02 | time/it: 2.14e-02     \n",
      "iter:   380 | dx: +3.6633e-05 | f1-score: 1.00 | relerr: 4.78e-02 | time/it: 2.14e-02     \n",
      "iter:   390 | dx: +3.9464e-05 | f1-score: 1.00 | relerr: 4.76e-02 | time/it: 2.18e-02     \n",
      "iter:   400 | dx: +3.5860e-05 | f1-score: 1.00 | relerr: 4.73e-02 | time/it: 2.16e-02     \n",
      "iter:   410 | dx: +3.7795e-05 | f1-score: 1.00 | relerr: 4.71e-02 | time/it: 2.16e-02     \n",
      "iter:   420 | dx: +3.8797e-05 | f1-score: 1.00 | relerr: 4.68e-02 | time/it: 2.18e-02     \n",
      "iter:   430 | dx: +3.5509e-05 | f1-score: 1.00 | relerr: 4.66e-02 | time/it: 2.16e-02     \n",
      "iter:   440 | dx: +3.6400e-05 | f1-score: 1.00 | relerr: 4.65e-02 | time/it: 2.31e-02     \n",
      "iter:   450 | dx: +3.3129e-05 | f1-score: 1.00 | relerr: 4.63e-02 | time/it: 2.16e-02     \n",
      "iter:   460 | dx: +4.9931e-05 | f1-score: 1.00 | relerr: 4.62e-02 | time/it: 2.28e-02     \n",
      "iter:   470 | dx: +3.8800e-05 | f1-score: 1.00 | relerr: 4.61e-02 | time/it: 5.41e-02     \n",
      "iter:   480 | dx: +3.9348e-05 | f1-score: 1.00 | relerr: 4.59e-02 | time/it: 4.97e-02     \n",
      "iter:   490 | dx: +3.2819e-05 | f1-score: 1.00 | relerr: 4.59e-02 | time/it: 2.58e-02     \n",
      "iter:   500 | dx: +3.8624e-05 | f1-score: 1.00 | relerr: 4.57e-02 | time/it: 4.16e-02     \n",
      "iter:   510 | dx: +4.2826e-05 | f1-score: 1.00 | relerr: 4.56e-02 | time/it: 3.16e-02     \n",
      "iter:   520 | dx: +3.4162e-05 | f1-score: 1.00 | relerr: 4.55e-02 | time/it: 2.74e-02     \n",
      "iter:   530 | dx: +3.1487e-05 | f1-score: 1.00 | relerr: 4.54e-02 | time/it: 2.91e-02     \n",
      "iter:   540 | dx: +3.4110e-05 | f1-score: 1.00 | relerr: 4.53e-02 | time/it: 2.29e-02     \n",
      "iter:   550 | dx: +3.5158e-05 | f1-score: 1.00 | relerr: 4.52e-02 | time/it: 2.30e-02     \n",
      "iter:   560 | dx: +4.1347e-05 | f1-score: 1.00 | relerr: 4.51e-02 | time/it: 2.47e-02     \n",
      "iter:   570 | dx: +3.8659e-05 | f1-score: 1.00 | relerr: 4.50e-02 | time/it: 2.28e-02     \n",
      "iter:   580 | dx: +3.1230e-05 | f1-score: 1.00 | relerr: 4.49e-02 | time/it: 2.28e-02     \n",
      "iter:   590 | dx: +2.4761e-05 | f1-score: 1.00 | relerr: 4.48e-02 | time/it: 2.28e-02     \n",
      "iter:   600 | dx: +2.9594e-05 | f1-score: 1.00 | relerr: 4.47e-02 | time/it: 2.26e-02     \n",
      "iter:   610 | dx: +3.8058e-05 | f1-score: 1.00 | relerr: 4.47e-02 | time/it: 2.28e-02     \n",
      "iter:   620 | dx: +4.2654e-05 | f1-score: 1.00 | relerr: 4.46e-02 | time/it: 2.30e-02     \n",
      "iter:   630 | dx: +2.5673e-05 | f1-score: 1.00 | relerr: 4.45e-02 | time/it: 2.31e-02     \n",
      "iter:   640 | dx: +3.0633e-05 | f1-score: 1.00 | relerr: 4.44e-02 | time/it: 2.29e-02     \n",
      "iter:   650 | dx: +3.8828e-05 | f1-score: 1.00 | relerr: 4.43e-02 | time/it: 2.29e-02     \n",
      "iter:   660 | dx: +4.5279e-05 | f1-score: 1.00 | relerr: 4.42e-02 | time/it: 2.28e-02     \n",
      "iter:   670 | dx: +4.5063e-05 | f1-score: 1.00 | relerr: 4.42e-02 | time/it: 2.32e-02     \n",
      "iter:   680 | dx: +4.7365e-05 | f1-score: 1.00 | relerr: 4.41e-02 | time/it: 2.31e-02     \n",
      "iter:   690 | dx: +4.1989e-05 | f1-score: 1.00 | relerr: 4.40e-02 | time/it: 2.36e-02     \n",
      "iter:   700 | dx: +3.9764e-05 | f1-score: 1.00 | relerr: 4.40e-02 | time/it: 2.32e-02     \n",
      "iter:   710 | dx: +3.6633e-05 | f1-score: 1.00 | relerr: 4.39e-02 | time/it: 2.64e-02     \n",
      "iter:   720 | dx: +4.6692e-05 | f1-score: 1.00 | relerr: 4.38e-02 | time/it: 3.80e-02     \n",
      "iter:   730 | dx: +4.1596e-05 | f1-score: 1.00 | relerr: 4.38e-02 | time/it: 4.01e-02     \n",
      "iter:   740 | dx: +5.2093e-05 | f1-score: 1.00 | relerr: 4.38e-02 | time/it: 2.33e-02     \n",
      "iter:   750 | dx: +4.5090e-05 | f1-score: 1.00 | relerr: 4.38e-02 | time/it: 3.60e-02     \n",
      "iter:   760 | dx: +2.0521e-05 | f1-score: 1.00 | relerr: 4.37e-02 | time/it: 3.38e-02     \n",
      "iter:   770 | dx: +3.2227e-05 | f1-score: 1.00 | relerr: 4.37e-02 | time/it: 2.33e-02     \n",
      "iter:   780 | dx: +2.7765e-05 | f1-score: 1.00 | relerr: 4.37e-02 | time/it: 2.44e-02     \n",
      "iter:   790 | dx: +3.1677e-05 | f1-score: 1.00 | relerr: 4.37e-02 | time/it: 2.51e-02     \n",
      "iter:   800 | dx: +4.5852e-05 | f1-score: 1.00 | relerr: 4.37e-02 | time/it: 2.96e-02     \n",
      "iter:   810 | dx: +4.5345e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.40e-02     \n",
      "iter:   820 | dx: +4.8364e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.63e-02     \n",
      "iter:   830 | dx: +3.3937e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.50e-02     \n",
      "iter:   840 | dx: +3.8136e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.40e-02     \n",
      "iter:   850 | dx: +2.7483e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.45e-02     \n",
      "iter:   860 | dx: +4.5593e-05 | f1-score: 1.00 | relerr: 4.36e-02 | time/it: 2.47e-02     \n",
      "iter:   870 | dx: +2.4445e-05 | f1-score: 1.00 | relerr: 4.35e-02 | time/it: 2.71e-02     \n",
      "iter:   880 | dx: +3.0460e-05 | f1-score: 1.00 | relerr: 4.35e-02 | time/it: 2.35e-02     \n",
      "iter:   890 | dx: +2.3053e-05 | f1-score: 1.00 | relerr: 4.35e-02 | time/it: 2.20e-02     \n",
      "iter:   900 | dx: +4.0632e-05 | f1-score: 1.00 | relerr: 4.35e-02 | time/it: 2.27e-02     \n",
      "iter:   910 | dx: +4.3953e-05 | f1-score: 1.00 | relerr: 4.34e-02 | time/it: 2.51e-02     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:   920 | dx: +3.6320e-05 | f1-score: 1.00 | relerr: 4.34e-02 | time/it: 4.92e-02     \n",
      "iter:   930 | dx: +2.3011e-05 | f1-score: 1.00 | relerr: 4.34e-02 | time/it: 4.22e-02     \n",
      "iter:   940 | dx: +3.8029e-05 | f1-score: 1.00 | relerr: 4.34e-02 | time/it: 2.86e-02     \n",
      "iter:   950 | dx: +3.8684e-05 | f1-score: 1.00 | relerr: 4.33e-02 | time/it: 3.14e-02     \n",
      "iter:   960 | dx: +4.0579e-05 | f1-score: 1.00 | relerr: 4.33e-02 | time/it: 2.88e-02     \n",
      "iter:   970 | dx: +3.7531e-05 | f1-score: 1.00 | relerr: 4.33e-02 | time/it: 2.53e-02     \n",
      "iter:   980 | dx: +3.1969e-05 | f1-score: 1.00 | relerr: 4.33e-02 | time/it: 2.43e-02     \n",
      "iter:   990 | dx: +4.0455e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.50e-02     \n",
      "iter:  1000 | dx: +3.4362e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.68e-02     \n",
      "iter:  1010 | dx: +3.9955e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.22e-02     \n",
      "iter:  1020 | dx: +2.4427e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.21e-02     \n",
      "iter:  1030 | dx: +4.3353e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.79e-02     \n",
      "iter:  1040 | dx: +2.7434e-05 | f1-score: 1.00 | relerr: 4.32e-02 | time/it: 2.49e-02     \n",
      "iter:  1050 | dx: +2.9248e-05 | f1-score: 1.00 | relerr: 4.31e-02 | time/it: 2.59e-02     \n",
      "iter:  1060 | dx: +3.7081e-05 | f1-score: 1.00 | relerr: 4.31e-02 | time/it: 2.95e-02     \n",
      "iter:  1070 | dx: +4.8607e-05 | f1-score: 1.00 | relerr: 4.31e-02 | time/it: 2.42e-02     \n",
      "iter:  1080 | dx: +3.9033e-05 | f1-score: 1.00 | relerr: 4.31e-02 | time/it: 2.65e-02     \n",
      "iter:  1090 | dx: +3.6899e-05 | f1-score: 1.00 | relerr: 4.30e-02 | time/it: 2.59e-02     \n",
      "iter:  1100 | dx: +3.6573e-05 | f1-score: 1.00 | relerr: 4.30e-02 | time/it: 2.69e-02     \n",
      "iter:  1110 | dx: +3.9073e-05 | f1-score: 1.00 | relerr: 4.29e-02 | time/it: 2.56e-02     \n",
      "iter:  1120 | dx: +4.4756e-05 | f1-score: 1.00 | relerr: 4.29e-02 | time/it: 2.58e-02     \n",
      "iter:  1130 | dx: +5.1576e-05 | f1-score: 1.00 | relerr: 4.29e-02 | time/it: 2.48e-02     \n",
      "iter:  1140 | dx: +5.8356e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.38e-02     \n",
      "iter:  1150 | dx: +4.3971e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.19e-02     \n",
      "iter:  1160 | dx: +2.8463e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.23e-02     \n",
      "iter:  1170 | dx: +2.6692e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.61e-02     \n",
      "iter:  1180 | dx: +3.1626e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.78e-02     \n",
      "iter:  1190 | dx: +3.2470e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.26e-02     \n",
      "iter:  1200 | dx: +3.2264e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.22e-02     \n",
      "iter:  1210 | dx: +3.8493e-05 | f1-score: 1.00 | relerr: 4.28e-02 | time/it: 2.39e-02     \n",
      "iter:  1220 | dx: +3.2836e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.24e-02     \n",
      "iter:  1230 | dx: +4.0075e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.51e-02     \n",
      "iter:  1240 | dx: +2.3372e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.31e-02     \n",
      "iter:  1250 | dx: +3.8201e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.47e-02     \n",
      "iter:  1260 | dx: +3.1267e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 2.32e-02     \n",
      "iter:  1270 | dx: +3.3592e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 3.04e-02     \n",
      "iter:  1280 | dx: +5.0918e-05 | f1-score: 1.00 | relerr: 4.27e-02 | time/it: 3.03e-02     \n",
      "Converged? True\n"
     ]
    }
   ],
   "source": [
    "dim = len(events)\n",
    "# Extract ground truth\n",
    "coeffs_true = np.hstack((coeffs_true_dict['baseline'],\n",
    "                         coeffs_true_dict['adjacency'].flatten()))\n",
    "# Set model\n",
    "model = tsvar.models.WoldModelVariationalFixedBeta(verbose=True)\n",
    "model.observe(events, beta=coeffs_true_dict['beta'])\n",
    "# Set priors\n",
    "as_pr = 0.1 * np.ones((dim + 1, dim))\n",
    "ar_pr = 1.0 * np.ones((dim + 1, dim))\n",
    "zc_pr = [1.0 * np.ones((len(events[i]), dim+1)) for i in range(dim)]\n",
    "# Set callback (parameters of callback are just the posterior mean of alpha)\n",
    "callback = tsvar.utils.callbacks.LearnerCallbackMLE(\n",
    "    x0=(as_pr / ar_pr).flatten(), print_every=PRINT_EVERY_VI,\n",
    "    coeffs_true=coeffs_true_dict['adjacency'].flatten(),\n",
    "    acc_thresh=0.05, dim=dim, default_end=CALLBACK_END)\n",
    "# Fit model\n",
    "conv = model.fit(as_pr=as_pr, ar_pr=ar_pr, zc_pr=zc_pr, max_iter=VI_FB_N_ITER,\n",
    "                 tol=1e-5, callback=callback)\n",
    "print('Converged?', conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======================================== WoldModelVariationalFixedBeta ======================================== \n",
      "\n",
      "Baseline:\n",
      "---------\n",
      "Ground truth:\n",
      "[ 0.05  0.03  0.03  0.01  0.01  0.03  0.02  0.04  0.01  0.05]\n",
      "Estimated:\n",
      "[ 0.04  0.03  0.03  0.00  0.01  0.02  0.02  0.05  0.01  0.05]\n",
      "\n",
      "Adjacency:\n",
      "---------\n",
      "Ground truth:\n",
      "[[ 0.00  0.18  0.00  0.18  0.13  0.14  0.12  0.13  0.00  0.17]\n",
      " [ 0.13  0.00  0.13  0.00  0.00  0.11  0.00  0.00  0.18  0.00]\n",
      " [ 0.11  0.00  0.00  0.00  0.11  0.19  0.19  0.16  0.00  0.11]\n",
      " [ 0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.16  0.00  0.12]\n",
      " [ 0.00  0.00  0.11  0.00  0.14  0.00  0.17  0.19  0.00  0.12]\n",
      " [ 0.00  0.17  0.00  0.00  0.17  0.00  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.12  0.00  0.00  0.13  0.00  0.12  0.00  0.00  0.16]\n",
      " [ 0.14  0.00  0.00  0.12  0.00  0.18  0.00  0.12  0.00  0.00]\n",
      " [ 0.00  0.00  0.20  0.00  0.00  0.12  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.19  0.16  0.19  0.18  0.00  0.00  0.17  0.20  0.00]]\n",
      "Estimated:\n",
      "[[ 0.00  0.18  0.00  0.19  0.14  0.16  0.13  0.11  0.02  0.16]\n",
      " [ 0.13  0.01  0.12  0.00  0.00  0.10  0.00  0.00  0.19  0.00]\n",
      " [ 0.12  0.00  0.00  0.00  0.11  0.18  0.19  0.15  0.00  0.11]\n",
      " [ 0.00  0.00  0.02  0.00  0.00  0.14  0.00  0.12  0.00  0.12]\n",
      " [ 0.00  0.00  0.11  0.00  0.15  0.00  0.15  0.19  0.00  0.12]\n",
      " [ 0.00  0.16  0.00  0.00  0.18  0.02  0.00  0.02  0.00  0.00]\n",
      " [ 0.00  0.13  0.00  0.00  0.15  0.00  0.10  0.00  0.00  0.17]\n",
      " [ 0.14  0.00  0.00  0.13  0.02  0.17  0.02  0.12  0.00  0.00]\n",
      " [ 0.00  0.01  0.19  0.00  0.00  0.14  0.01  0.00  0.00  0.01]\n",
      " [ 0.00  0.20  0.16  0.19  0.16  0.01  0.00  0.15  0.19  0.01]]\n",
      "True Positive: 43\n",
      "True Negative: 57\n",
      "False Positive: 0\n",
      "False Negative: 0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Beta:\n",
      "-----\n",
      " -- IS FIXED --\n"
     ]
    }
   ],
   "source": [
    "print('\\n', '='*40, type(model).__name__, '='*40, '\\n')\n",
    "\n",
    "alpha_hat_mean = np.round(model._as_po / model._ar_po, 2)\n",
    "\n",
    "baseline_hat = alpha_hat_mean[0,:]\n",
    "print('Baseline:')\n",
    "print('---------')\n",
    "print('Ground truth:')\n",
    "print(mu.numpy())\n",
    "print('Estimated:')\n",
    "print(baseline_hat)\n",
    "print()\n",
    "\n",
    "adj_true = alpha.numpy()\n",
    "adjacency_hat = alpha_hat_mean[1:,:]\n",
    "print('Adjacency:')\n",
    "print('---------')\n",
    "print('Ground truth:')\n",
    "print(adj_true)\n",
    "print('Estimated:')\n",
    "print(adjacency_hat)\n",
    "\n",
    "THRESH = 0.05\n",
    "fp = tsvar.utils.metrics.false_positive(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "fn = tsvar.utils.metrics.false_negative(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "tp = tsvar.utils.metrics.true_positive(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "tn = tsvar.utils.metrics.true_negative(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "fscore = tsvar.utils.metrics.fscore(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "print('True Positive:', tp)\n",
    "print('True Negative:', tn)\n",
    "print('False Positive:', fp)\n",
    "print('False Negative:', fn)\n",
    "print('F1-Score:', fscore)\n",
    "print()\n",
    "\n",
    "print('Beta:')\n",
    "print('-----')\n",
    "print(' -- IS FIXED --')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.grid()\n",
    "plt.boxplot(\n",
    "    [\n",
    "        adjacency_hat[adj_true == 0.0], \n",
    "        adjacency_hat[adj_true > 0.0]\n",
    "    ],\n",
    "    labels=[\n",
    "        'True Non-edges',\n",
    "        'True Edges'\n",
    "    ])\n",
    "plt.ylabel(r'Learned value of paramaters')\n",
    "plt.title('Distribution of learned adjacency weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test VI\n",
    "\n",
    "Algorithm: Mean-Field VI with Variable $\\{\\beta\\}$s using `WoldModelVariational`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gb\n",
    "import time\n",
    "\n",
    "dim = len(events)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "GB_N_ITER = 300\n",
    "\n",
    "# Define model\n",
    "granger_model = gb.GrangerBusca(\n",
    "    alpha_prior=1.0/len(events),\n",
    "    num_iter=GB_N_ITER,\n",
    "    metropolis=True,\n",
    "    beta_strategy='busca',\n",
    "    num_jobs=1,\n",
    ")\n",
    "start_time = time.time()\n",
    "granger_model.fit(events)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "# Extract infered adjacency\n",
    "adj_hat = granger_model.Alpha_.toarray()\n",
    "adj_hat = adj_hat / adj_hat.sum(axis=1)\n",
    "beta_hat = np.ones((dim, dim)) * (granger_model.beta_ + 1)\n",
    "coeffs_hat = np.hstack((granger_model.mu_, beta_hat.flatten(),\n",
    "                        adj_hat.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency:\n",
      "---------\n",
      "Ground truth:\n",
      "[[ 0.00  0.18  0.00  0.18  0.13  0.14  0.12  0.13  0.00  0.17]\n",
      " [ 0.13  0.00  0.13  0.00  0.00  0.11  0.00  0.00  0.18  0.00]\n",
      " [ 0.11  0.00  0.00  0.00  0.11  0.19  0.19  0.16  0.00  0.11]\n",
      " [ 0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.16  0.00  0.12]\n",
      " [ 0.00  0.00  0.11  0.00  0.14  0.00  0.17  0.19  0.00  0.12]\n",
      " [ 0.00  0.17  0.00  0.00  0.17  0.00  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.12  0.00  0.00  0.13  0.00  0.12  0.00  0.00  0.16]\n",
      " [ 0.14  0.00  0.00  0.12  0.00  0.18  0.00  0.12  0.00  0.00]\n",
      " [ 0.00  0.00  0.20  0.00  0.00  0.12  0.00  0.00  0.00  0.00]\n",
      " [ 0.00  0.19  0.16  0.19  0.18  0.00  0.00  0.17  0.20  0.00]]\n",
      "Estimated:\n",
      "[[ 0.18  0.17  0.00  0.17  0.09  0.00  0.12  0.00  0.00  0.21]\n",
      " [ 0.20  0.37  0.12  0.00  0.04  0.05  0.00  0.01  0.27  0.00]\n",
      " [ 0.19  0.02  0.22  0.00  0.05  0.05  0.28  0.14  0.00  0.03]\n",
      " [ 0.00  0.00  0.01  0.31  0.00  0.16  0.00  0.18  0.00  0.16]\n",
      " [ 0.00  0.00  0.13  0.00  0.54  0.00  0.07  0.24  0.00  0.03]\n",
      " [ 0.01  0.29  0.00  0.00  0.17  0.49  0.00  0.04  0.00  0.00]\n",
      " [ 0.00  0.11  0.00  0.01  0.03  0.00  0.39  0.00  0.00  0.37]\n",
      " [ 0.31  0.00  0.00  0.18  0.00  0.16  0.00  0.49  0.00  0.00]\n",
      " [ 0.00  0.00  0.28  0.00  0.03  0.17  0.00  0.06  0.22  0.00]\n",
      " [ 0.00  0.03  0.21  0.14  0.15  0.00  0.00  0.08  0.20  0.28]]\n",
      "\n",
      "F1-Score: 1.0000\n",
      "Rel. Err: 4.52e-02\n",
      "\n",
      "Diff with last call:\n",
      "sum: 2.1267857993357433\n",
      "max: 0.13650430180988665\n",
      "# diff edges: 8\n"
     ]
    }
   ],
   "source": [
    "adj_true = alpha.numpy()\n",
    "print('Adjacency:')\n",
    "print('---------')\n",
    "print('Ground truth:')\n",
    "print(adj_true)\n",
    "print('Estimated:')\n",
    "print(adj_hat)\n",
    "print()\n",
    "fscore = tsvar.utils.metrics.fscore(adjacency_hat.flatten(), adj_true.flatten(), threshold=THRESH)\n",
    "relerr = tsvar.utils.metrics.relerr(adjacency_hat.flatten(), adj_true.flatten())\n",
    "print(f'F1-Score: {fscore:.4f}')\n",
    "print(f'Rel. Err: {relerr:.2e}')\n",
    "print()\n",
    "print('Diff with last call:')\n",
    "print('sum:', np.sum(np.abs(adj_hat - last_adj)))\n",
    "print('max:', np.max(np.abs(adj_hat - last_adj)))\n",
    "print('# diff edges:', np.sum((adj_hat > 0.05) ^ (last_adj > 0.05)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_adj = adj_hat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
