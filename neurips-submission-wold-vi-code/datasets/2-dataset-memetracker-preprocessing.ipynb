{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemeTracker Dataset\n",
    "\n",
    "The following notebook contains the documented code used to preprocess the MemeTracker dataset for our experiments. \n",
    "\n",
    "**Note:** Because the dataset is quite large, a machine with at least ~40GB of RAM is necessary to run this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from tsvar.preprocessing import Dataset\n",
    "\n",
    "# Set larger cell width for nicer visualization\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and parse the dataset\n",
    "\n",
    "First, all the files from the dataset must be downloaded from the [SNAP](http://snap.stanford.edu/data/memetracker9.html) dataset repository.\n",
    "\n",
    "Then the raw files must be parsed using the `raw2df.py` script provided by [NPHC](https://github.com/achab/nphc/tree/master/nphc/datasets/memetracker) to format the raw data in a convenient tabular format.\n",
    "\n",
    "The point processes can then be built using the following notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the raw MemeTracker dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the input directory where the parsed dataframes are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './parsed_memetracker_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw dataframes (in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_files = sorted(glob.glob(os.path.join(DATA_DIR, 'parsed', 'df_*.csv')))\n",
    "\n",
    "def worker(fname):\n",
    "    return pd.read_csv(fname)\n",
    "\n",
    "pool = multiprocessing.Pool(len(list_df_files))\n",
    "\n",
    "jobs = list()\n",
    "for fname in list_df_files:\n",
    "    job = pool.apply_async(worker, (fname, ))\n",
    "    jobs.append(job)\n",
    "\n",
    "data = list()\n",
    "for job in jobs:\n",
    "    data.append(job.get())\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "del data\n",
    "\n",
    "pool.close()\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Blog` = receiver\n",
    "* `Hyperlink` = sender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Clean the dataframe\n",
    "\n",
    "### 3.1. Clean columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the `Hyperlink column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hyperlink'] = df['Hyperlink'].str.strip()  # Remove whitespaces (that appear in null hyperlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast `Date` and build timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Timestamp'] = df['Date'].values.astype(np.int64) // (10 ** 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Find the top-100 blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_hyperlink'] = df.Hyperlink != ''  # Indicate if event has hyperlink\n",
    "df['has_hyperlink'] = df['has_hyperlink'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the count of number of hyperlink per blogs, i.e., how many times a blog was cited."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count_series = df.groupby('Hyperlink').agg({'Hyperlink': 'count'})['Hyperlink']\n",
    "count_series = count_series.loc[count_series.index != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the count of number of posts per blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_series = df.groupby('Blog').agg({'PostNb': set})['PostNb'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the top-100 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_num = 100\n",
    "\n",
    "top_series = count_series.sort_values(ascending=False).iloc[:top_num]\n",
    "print(f'There are {top_series.sum():,d} items in the top-{top_num:d} sites')\n",
    "display(top_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Keep only events between sites in the top-100 blogs\n",
    "\n",
    "We finally remove all events coming from hyperlinks that are not part of the top-100 blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_site_set = set(top_series.index.tolist())  # All top blog sites\n",
    "\n",
    "top_blog_mask = df['Blog'].isin(top_site_set)     # Blogs is in top\n",
    "top_hp_mask = df['Hyperlink'].isin(set(list(top_site_set) + ['']))  # Hyperlink is in top or no hyperlink (i.e. is null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build mask of valid events\n",
    "valid_event_mask = top_blog_mask & top_hp_mask\n",
    "\n",
    "# Filter\n",
    "df_top = df.loc[valid_event_mask]\n",
    "assert len(df_top) == np.sum(valid_event_mask)\n",
    "\n",
    "print(f'{np.sum(valid_event_mask):,d} events are between the top-{top_num} sites'\n",
    "      f' out of the {len(df):,d} ({np.sum(valid_event_mask)*100/len(df):.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Final formatting steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build numerical index for each blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_name_to_idx_map = dict(zip(top_series.index, range(top_num)))\n",
    "\n",
    "# Make numerical index for blogs\n",
    "df_top['Blog_idx'] = df_top['Blog'].apply(lambda name: top_name_to_idx_map[name])\n",
    "\n",
    "# Add hyperlinks index\n",
    "top_name_to_idx_map[''] = None  # Set None for No-Hyperlink\n",
    "df_top['Hyperlink_idx'] = df_top['Hyperlink'].apply(lambda name: top_name_to_idx_map[name]).astype(pd.Int32Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = df_top.sort_values(by='Timestamp')  # Translate time origin\n",
    "df_top['Timestamp'] -= df_top['Timestamp'].min()  # Translate time origin\n",
    "df_top = df_top[['Hyperlink_idx', 'Blog_idx', 'Hyperlink', 'Blog', 'Date', 'Timestamp']]  # df_top = df_top[['Hyperlink_idx', 'Blog_idx', 'Hyperlink', 'Blog', 'Date', 'Timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_top.shape)\n",
    "display(df_top.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top.to_pickle(os.path.join(DATA_DIR, 'memetracker-top100-clean.pickle.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
