{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import tsvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random parameters\n",
    "dim = 2  # Dimensionality of the process\n",
    "max_jumps=50e3 * dim  # Number of events\n",
    "\n",
    "mu = torch.tensor([0.3, 0.1])\n",
    "beta = torch.tensor([\n",
    "    [1.0, 0.2],\n",
    "    [0.5, 0.1]\n",
    "])\n",
    "# Use the same constraint as GrangerBusca to allow fair comparison\n",
    "alpha = torch.tensor([\n",
    "    [0.7, 0.3],\n",
    "    [0.0, 1.0]\n",
    "])\n",
    "\n",
    "param_dict = {\n",
    "    'baseline': mu.tolist(),\n",
    "    'beta': beta.tolist(),\n",
    "    'adjacency': alpha.tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments_utils import generate_parameters\n",
    "\n",
    "dim = 10\n",
    "max_jumps=20e3 * dim\n",
    "\n",
    "param_dict = generate_parameters(dim=dim)\n",
    "\n",
    "mu = torch.tensor(param_dict['baseline'])\n",
    "beta = torch.tensor(param_dict['beta'])\n",
    "alpha = torch.tensor(param_dict['adjacency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Simulate lots of data...\n",
      "    - Simulated 200,000 events with end time: 129328.7109375\n",
      "    - Events:\n",
      "      - dim  0 (15641 events): [      1.43       8.66      56.05 ...  129320.01  129320.09  129324.05]\n",
      "      - dim  1 (26173 events): [     14.10      26.37      42.33 ...  129313.47  129317.32  129318.38]\n",
      "      - dim  2 (22957 events): [     44.90      45.68      53.20 ...  129323.62  129326.46  129326.78]\n",
      "      - dim  3 (28957 events): [     27.57      53.14      54.86 ...  129314.80  129325.15  129328.71]\n",
      "      - dim  4 (14942 events): [      7.24      42.18      63.09 ...  129312.20  129314.23  129316.34]\n",
      "      - dim  5 (17449 events): [     22.73      75.95      82.20 ...  129310.24  129322.94  129326.55]\n",
      "      - dim  6 (16937 events): [     77.06      80.25     103.82 ...  129317.19  129318.50  129319.31]\n",
      "      - dim  7 (12699 events): [    615.59     657.96     658.23 ...  129296.70  129299.62  129306.08]\n",
      "      - dim  8 (19677 events): [     27.84      29.61      39.34 ...  129302.60  129313.33  129313.91]\n",
      "      - dim  9 (24568 events): [      3.67      34.05      73.34 ...  129311.98  129312.16  129327.91]\n"
     ]
    }
   ],
   "source": [
    "coeffs_true = torch.cat((mu, beta.flatten(), alpha.flatten())).numpy()\n",
    "print('  - Simulate lots of data...')\n",
    "# Simulate lots of data\n",
    "wold_sim = tsvar.simulate.MultivariateWoldSimulator(\n",
    "    mu_a=mu, alpha_ba=alpha, beta_ba=beta)\n",
    "events = wold_sim.simulate(max_jumps=max_jumps, seed=None)\n",
    "events = [torch.tensor(ev, dtype=torch.float) for ev in events]\n",
    "end_time = max(map(max, events))\n",
    "print((f\"    - Simulated {sum(map(len, events)):,d} events \"\n",
    "       f\"with end time: {end_time}\"))\n",
    "print(\"    - Events:\")\n",
    "for i, events_i in enumerate(events):\n",
    "    print(f\"      - dim {i:>2d} ({len(events_i):>5d} events):\", events_i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:    50 | dx: +1.2999e-01 | loss: 2.2196e+01 | dloss: +1.75e-02 | acc: 0.53 | f1-score: 0.59 | relerr: 21.47 | time/it: 1.23e-01     \n",
      "iter:   100 | dx: +2.2845e-02 | loss: 2.2325e+01 | dloss: -1.17e-03 | acc: 0.53 | f1-score: 0.59 | relerr: 21.42 | time/it: 1.05e-01     \n",
      "iter:   150 | dx: +3.0869e-02 | loss: 2.2252e+01 | dloss: -1.68e-03 | acc: 0.53 | f1-score: 0.59 | relerr: 21.08 | time/it: 1.08e-01     \n",
      "iter:   200 | dx: +4.7185e-02 | loss: 2.2156e+01 | dloss: -2.16e-03 | acc: 0.53 | f1-score: 0.59 | relerr: 20.65 | time/it: 1.05e-01     \n",
      "iter:   250 | dx: +9.6080e-02 | loss: 2.2031e+01 | dloss: -2.94e-03 | acc: 0.53 | f1-score: 0.59 | relerr: 20.13 | time/it: 1.05e-01     \n",
      "iter:   300 | dx: +8.9695e-02 | loss: 2.1889e+01 | dloss: -2.92e-03 | acc: 0.54 | f1-score: 0.60 | relerr: 19.62 | time/it: 1.06e-01     \n",
      "iter:   350 | dx: +1.3149e-01 | loss: 2.1722e+01 | dloss: -3.58e-03 | acc: 0.55 | f1-score: 0.60 | relerr: 19.11 | time/it: 1.05e-01     \n",
      "iter:   400 | dx: +1.9148e-01 | loss: 2.1562e+01 | dloss: -1.32e-03 | acc: 0.56 | f1-score: 0.59 | relerr: 18.82 | time/it: 1.09e-01     \n",
      "iter:   450 | dx: +1.4873e-04 | loss: 2.1530e+01 | dloss: -3.52e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.83 | time/it: 1.02e-01     \n",
      "iter:   500 | dx: +1.6714e-04 | loss: 2.1512e+01 | dloss: -3.74e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.83 | time/it: 9.11e-02     \n",
      "iter:   550 | dx: +1.7735e-04 | loss: 2.1493e+01 | dloss: -3.95e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.83 | time/it: 9.06e-02     \n",
      "iter:   600 | dx: +1.8705e-04 | loss: 2.1472e+01 | dloss: -4.17e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.83 | time/it: 8.96e-02     \n",
      "iter:   650 | dx: +1.9666e-04 | loss: 2.1451e+01 | dloss: -4.37e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.83 | time/it: 9.05e-02     \n",
      "iter:   700 | dx: +2.0619e-04 | loss: 2.1429e+01 | dloss: -4.58e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.99e-02     \n",
      "iter:   750 | dx: +2.1567e-04 | loss: 2.1405e+01 | dloss: -4.78e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.94e-02     \n",
      "iter:   800 | dx: +2.2512e-04 | loss: 2.1381e+01 | dloss: -4.98e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.14e-02     \n",
      "iter:   850 | dx: +2.3457e-04 | loss: 2.1355e+01 | dloss: -5.18e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.04e-02     \n",
      "iter:   900 | dx: +2.4402e-04 | loss: 2.1329e+01 | dloss: -5.38e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.20e-02     \n",
      "iter:   950 | dx: +2.5351e-04 | loss: 2.1302e+01 | dloss: -5.57e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.18e-02     \n",
      "iter:  1000 | dx: +2.6305e-04 | loss: 2.1273e+01 | dloss: -5.77e-04 | acc: 0.55 | f1-score: 0.58 | relerr: 18.82 | time/it: 8.15e-02     \n",
      "\n",
      "Converged? False\n",
      "  - coeffs_hat:  [  2.70   2.67   2.64   2.67   2.59   2.63   2.63   2.67   2.67   2.67   0.00  15.74   0.00  15.59  15.71   0.00   0.00  15.73  15.91  15.86   0.00  16.83   0.00  16.90   0.00   0.00   0.00  17.14  17.07  16.88   0.00  16.65   0.00  16.58   0.00   0.00   0.00  16.82  16.72  16.71   0.00  16.99   0.00  17.19   0.00   0.00   0.00  16.98  17.07  17.04   0.00  15.56   0.00  15.50   0.00   0.00   0.00\n",
      "  15.82  15.46  15.48   0.00  16.05   0.00  15.95   0.00   0.00   0.00  15.80  16.04  15.96   0.00  15.99   0.00  15.91   0.00   0.00   0.00  15.79  16.11  16.10   0.00  15.20   0.00  14.95   0.00   0.00   0.00  15.55  14.98  15.13   0.00  16.14   0.00  16.20   0.00   0.00   0.00  16.49  16.55  16.41   0.00  16.76   0.00  16.76  16.67   0.00   0.00  17.03  16.95  16.72   0.00   2.67   0.00   2.67\n",
      "   2.61   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00\n",
      "   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   0.00   0.00   0.00   2.67   2.67   2.67   0.00   2.67   0.00   2.67   2.62   0.00   0.00   2.67   2.67   2.67]\n",
      "  - coeffs_true: [ 0.02  0.02  0.03  0.04  0.02  0.03  0.03  0.00  0.04  0.04  0.36  0.62  0.08  0.37  0.93  0.65  0.40  0.79  0.32  0.57  0.87  0.44  0.80  0.14  0.70  0.70  0.22  0.92  0.44  0.91  0.06  0.18  0.05  0.67  0.59  0.53  0.04  0.56  0.33  0.50  0.11  0.61  0.57  0.01  0.62  0.91  0.79  0.99  0.96  0.79  0.29  0.62  0.48  0.20  0.38  0.05  0.45  0.98  0.12  0.12  0.74  0.59  0.47  0.11  0.23  0.90\n",
      "  0.42  0.54  0.01  0.30  0.44  0.61  0.92  0.63  0.71  0.15  0.75  0.83  0.63  0.44  0.15  0.57  0.53  0.95  0.48  0.50  0.54  0.82  0.06  0.67  0.77  0.71  0.80  0.56  0.97  0.15  0.03  0.59  0.11  0.95  0.33  0.19  0.46  0.92  0.88  0.25  0.35  0.18  0.90  0.71  0.11  0.05  0.19  0.18  0.00  0.00  0.00  0.08  0.00  0.00  0.00  0.15  0.16  0.00  0.00  0.00  0.11  0.00  0.00  0.11  0.00  0.06\n",
      "  0.07  0.14  0.00  0.00  0.06  0.00  0.05  0.00  0.10  0.16  0.00  0.00  0.13  0.15  0.08  0.17  0.14  0.13  0.00  0.15  0.09  0.17  0.18  0.16  0.15  0.00  0.18  0.15  0.10  0.00  0.00  0.05  0.00  0.18  0.00  0.15  0.06  0.11  0.06  0.00  0.13  0.08  0.00  0.00  0.00  0.16  0.00  0.00  0.09  0.00  0.00  0.20  0.00  0.00  0.14  0.00  0.14  0.07  0.09  0.20  0.11  0.14  0.13  0.08  0.00  0.00\n",
      "  0.00  0.00  0.00  0.08  0.00  0.07  0.07  0.00  0.00  0.00  0.00  0.15]\n",
      "  - max_diff: 17.1791\n"
     ]
    }
   ],
   "source": [
    "# coeffs_true_dict = param_dict\n",
    "seed = 1234\n",
    "\n",
    "MLE_N_ITER = 75000\n",
    "\n",
    "PRINT_EVERY = 50\n",
    "CALLBACK_END = '\\n'\n",
    "\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set initial guess\n",
    "coeffs_start = torch.tensor(np.hstack((\n",
    "    np.random.uniform(0.0, 1.0, size=dim),     # baseline\n",
    "    np.random.uniform(0.0, 1.0, size=dim**2),  # beta\n",
    "    np.random.uniform(0.0, 1.0, size=dim**2)   # adjacency\n",
    ")))\n",
    "\n",
    "# Extract ground truth\n",
    "coeffs_true = np.hstack((coeffs_true_dict['baseline'],\n",
    "                         np.array(coeffs_true_dict['beta']).flatten(),\n",
    "                         np.array(coeffs_true_dict['adjacency']).flatten()))\n",
    "\n",
    "# Define model\n",
    "model = tsvar.models.WoldModelMLE(verbose=True)\n",
    "model.observe(events, end_time)\n",
    "\n",
    "# Set callback\n",
    "callback = tsvar.utils.callbacks.LearnerCallbackMLE(\n",
    "    coeffs_start, print_every=PRINT_EVERY, coeffs_true=coeffs_true,\n",
    "    acc_thresh=0.05, dim=dim, default_end=CALLBACK_END)\n",
    "\n",
    "# Fit model\n",
    "conv = model.fit(x0=coeffs_start, optimizer=torch.optim.Adam, lr=0.5,\n",
    "                 lr_sched=0.9999, tol=1e-5, max_iter=1000,\n",
    "                 penalty=tsvar.priors.GaussianPrior, C=1e4,\n",
    "                 seed=None, callback=callback)\n",
    "coeffs_hat = model.coeffs.detach().numpy()\n",
    "\n",
    "# Print results\n",
    "print('\\nConverged?', conv)\n",
    "max_diff = np.max(np.abs(coeffs_true - coeffs_hat))\n",
    "print(f'  - coeffs_hat:  {coeffs_hat.round(2)}')\n",
    "print(f'  - coeffs_true: {coeffs_true.round(2)}')\n",
    "print(f'  - max_diff: {max_diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00,  15.74,   0.00,  15.59,  15.71,   0.00,   0.00,  15.73,  15.91,  15.86],\n",
       "       [  0.00,  16.83,   0.00,  16.90,   0.00,   0.00,   0.00,  17.14,  17.07,  16.88],\n",
       "       [  0.00,  16.65,   0.00,  16.58,   0.00,   0.00,   0.00,  16.82,  16.72,  16.71],\n",
       "       [  0.00,  16.99,   0.00,  17.19,   0.00,   0.00,   0.00,  16.98,  17.07,  17.04],\n",
       "       [  0.00,  15.56,   0.00,  15.50,   0.00,   0.00,   0.00,  15.82,  15.46,  15.48],\n",
       "       [  0.00,  16.05,   0.00,  15.95,   0.00,   0.00,   0.00,  15.80,  16.04,  15.96],\n",
       "       [  0.00,  15.99,   0.00,  15.91,   0.00,   0.00,   0.00,  15.79,  16.11,  16.10],\n",
       "       [  0.00,  15.20,   0.00,  14.95,   0.00,   0.00,   0.00,  15.55,  14.98,  15.13],\n",
       "       [  0.00,  16.14,   0.00,  16.20,   0.00,   0.00,   0.00,  16.49,  16.55,  16.41],\n",
       "       [  0.00,  16.76,   0.00,  16.76,  16.67,   0.00,   0.00,  17.03,  16.95,  16.72]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(coeffs_hat[dim:dim+dim**2], (dim, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
